{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Trabajo 3: Clasificación de Imágenes Médicas (Pneumonia Detection)\n",
                "## Parte 2: Clasificación con Descriptores Clásicos y CNN\n",
                "**Objetivo:** Entrenar y evaluar múltiples clasificadores utilizando descriptores clásicos (HOG) y una red neuronal convolucional (CNN).\n",
                "\n",
                "**Tareas:**\n",
                "1. Crear matriz de características (HOG).\n",
                "2. Normalización y Reducción de Dimensionalidad (PCA).\n",
                "3. Entrenar clasificadores: SVM, Random Forest, k-NN, Logistic Regression.\n",
                "4. Entrenar una CNN básica.\n",
                "5. Evaluación comparativa."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import cv2\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "from skimage.feature import hog\n",
                "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, classification_report\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras import layers, models\n",
                "\n",
                "%matplotlib inline\n",
                "plt.rcParams['figure.figsize'] = (10, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Carga y Preprocesamiento de Datos\n",
                "Cargamos las imágenes, las convertimos a escala de grises, redimensionamos a 224x224 y aplicamos CLAHE."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BASE_DIR = Path('../data')\n",
                "TRAIN_DIR = BASE_DIR / 'train'\n",
                "TEST_DIR = BASE_DIR / 'test'\n",
                "VAL_DIR = BASE_DIR / 'val'\n",
                "\n",
                "IMG_SIZE = (224, 224)\n",
                "\n",
                "def load_and_preprocess_data(directory):\n",
                "    images = []\n",
                "    labels = []\n",
                "    classes = ['NORMAL', 'PNEUMONIA']\n",
                "    \n",
                "    for label, class_name in enumerate(classes):\n",
                "        path = directory / class_name\n",
                "        if not path.exists():\n",
                "            print(f'Warning: Path {path} does not exist.')\n",
                "            continue\n",
                "            \n",
                "        # Buscar extensiones comunes\n",
                "        files = list(path.glob('*.jpeg')) + list(path.glob('*.jpg')) + list(path.glob('*.png'))\n",
                "        \n",
                "        for img_path in files:\n",
                "            # Leer en escala de grises\n",
                "            img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
                "            if img is None: continue\n",
                "            \n",
                "            # Resize\n",
                "            img = cv2.resize(img, IMG_SIZE)\n",
                "            \n",
                "            # CLAHE\n",
                "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
                "            img = clahe.apply(img)\n",
                "            \n",
                "            images.append(img)\n",
                "            labels.append(label)\n",
                "            \n",
                "    return np.array(images), np.array(labels)\n",
                "\n",
                "print('Cargando datos de entrenamiento...')\n",
                "X_train_raw, y_train = load_and_preprocess_data(TRAIN_DIR)\n",
                "print('Cargando datos de validación...')\n",
                "X_val_raw, y_val = load_and_preprocess_data(VAL_DIR)\n",
                "print('Cargando datos de prueba...')\n",
                "X_test_raw, y_test = load_and_preprocess_data(TEST_DIR)\n",
                "\n",
                "print(f'Train shape: {X_train_raw.shape}')\n",
                "print(f'Val shape: {X_val_raw.shape}')\n",
                "print(f'Test shape: {X_test_raw.shape}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Extracción de Características (HOG)\n",
                "Utilizamos Histogram of Oriented Gradients (HOG) como descriptor clásico."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_hog_features(images):\n",
                "    hog_features = []\n",
                "    print('Extrayendo características HOG... (esto puede tardar un poco)')\n",
                "    for i, img in enumerate(images):\n",
                "        if i % 500 == 0: print(f'Procesando imagen {i}/{len(images)}')\n",
                "        # pixels_per_cell=(16, 16) reduce la dimensionalidad\n",
                "        feature = hog(img, orientations=9, pixels_per_cell=(16, 16),\n",
                "                      cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False)\n",
                "        hog_features.append(feature)\n",
                "    return np.array(hog_features)\n",
                "\n",
                "X_train_hog = extract_hog_features(X_train_raw)\n",
                "X_val_hog = extract_hog_features(X_val_raw)\n",
                "X_test_hog = extract_hog_features(X_test_raw)\n",
                "\n",
                "print(f'HOG Train shape: {X_train_hog.shape}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Normalización y Reducción de Dimensionalidad (PCA)\n",
                "Combinamos Train y Val para el entrenamiento de modelos clásicos (para tener más datos en CV), normalizamos y aplicamos PCA para reducir el número de features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combinar Train y Val\n",
                "X_full_hog = np.concatenate((X_train_hog, X_val_hog))\n",
                "y_full = np.concatenate((y_train, y_val))\n",
                "\n",
                "# Normalización\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X_full_hog)\n",
                "X_test_scaled = scaler.transform(X_test_hog)\n",
                "\n",
                "# PCA\n",
                "print('Aplicando PCA...')\n",
                "pca = PCA(n_components=0.95) # Mantener 95% de varianza\n",
                "X_pca = pca.fit_transform(X_scaled)\n",
                "X_test_pca = pca.transform(X_test_scaled)\n",
                "\n",
                "print(f'Original features: {X_scaled.shape[1]}')\n",
                "print(f'Reduced features (95% variance): {X_pca.shape[1]}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Entrenamiento y Evaluación de Clasificadores Clásicos\n",
                "Probamos SVM, Random Forest, k-NN y Regresión Logística con Validación Cruzada."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "classifiers = {\n",
                "    'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=42),\n",
                "    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
                "    'k-NN': KNeighborsClassifier(n_neighbors=5),\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
                "}\n",
                "\n",
                "results = {}\n",
                "\n",
                "for name, clf in classifiers.items():\n",
                "    print(f'\\nEntrenando {name}...')\n",
                "    # Cross Validation\n",
                "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "    cv_scores = cross_val_score(clf, X_pca, y_full, cv=cv, scoring='f1')\n",
                "    print(f'  CV F1-Score: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})')\n",
                "    \n",
                "    # Train on full train set\n",
                "    clf.fit(X_pca, y_full)\n",
                "    \n",
                "    # Predict on Test\n",
                "    y_pred = clf.predict(X_test_pca)\n",
                "    y_prob = clf.predict_proba(X_test_pca)[:, 1] if hasattr(clf, 'predict_proba') else None\n",
                "    \n",
                "    # Metrics\n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    prec = precision_score(y_test, y_pred)\n",
                "    rec = recall_score(y_test, y_pred)\n",
                "    f1 = f1_score(y_test, y_pred)\n",
                "    \n",
                "    results[name] = {\n",
                "        'Accuracy': acc,\n",
                "        'Precision': prec,\n",
                "        'Recall': rec,\n",
                "        'F1-Score': f1,\n",
                "        'y_pred': y_pred,\n",
                "        'y_prob': y_prob\n",
                "    }\n",
                "    \n",
                "    print(f'  Test F1-Score: {f1:.4f}')\n",
                "    \n",
                "    # Feature Importance (only for RF)\n",
                "    if name == 'Random Forest':\n",
                "        importances = clf.feature_importances_\n",
                "        indices = np.argsort(importances)[::-1]\n",
                "        plt.figure(figsize=(10, 4))\n",
                "        plt.title('Feature Importances (Top 20 PCA Components)')\n",
                "        plt.bar(range(20), importances[indices[:20]], align='center')\n",
                "        plt.xticks(range(20), indices[:20])\n",
                "        plt.xlim([-1, 20])\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Convolutional Neural Network (CNN)\n",
                "Entrenamos una CNN básica utilizando las imágenes en bruto (preprocesadas)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Preparando datos para CNN...')\n",
                "# Reshape for CNN: (N, 224, 224, 1)\n",
                "X_train_cnn = X_train_raw.reshape(-1, 224, 224, 1) / 255.0\n",
                "X_val_cnn = X_val_raw.reshape(-1, 224, 224, 1) / 255.0\n",
                "X_test_cnn = X_test_raw.reshape(-1, 224, 224, 1) / 255.0\n",
                "\n",
                "model = models.Sequential([\n",
                "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),\n",
                "    layers.MaxPooling2D((2, 2)),\n",
                "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
                "    layers.MaxPooling2D((2, 2)),\n",
                "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
                "    layers.Flatten(),\n",
                "    layers.Dense(64, activation='relu'),\n",
                "    layers.Dropout(0.5),\n",
                "    layers.Dense(1, activation='sigmoid')\n",
                "])\n",
                "\n",
                "model.compile(optimizer='adam',\n",
                "              loss='binary_crossentropy',\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "print('Entrenando CNN...')\n",
                "history = model.fit(X_train_cnn, y_train, epochs=10, \n",
                "                    validation_data=(X_val_cnn, y_val), batch_size=32)\n",
                "\n",
                "# Evaluate CNN\n",
                "y_prob_cnn = model.predict(X_test_cnn)\n",
                "y_pred_cnn = (y_prob_cnn > 0.5).astype(int).flatten()\n",
                "\n",
                "results['CNN'] = {\n",
                "    'Accuracy': accuracy_score(y_test, y_pred_cnn),\n",
                "    'Precision': precision_score(y_test, y_pred_cnn),\n",
                "    'Recall': recall_score(y_test, y_pred_cnn),\n",
                "    'F1-Score': f1_score(y_test, y_pred_cnn),\n",
                "    'y_pred': y_pred_cnn,\n",
                "    'y_prob': y_prob_cnn.flatten()\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Comparación de Resultados\n",
                "Visualizamos las métricas y curvas ROC de todos los modelos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare Metrics\n",
                "metrics_df = pd.DataFrame(results).T[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
                "print(metrics_df)\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.heatmap(metrics_df, annot=True, cmap='viridis', fmt='.4f')\n",
                "plt.title('Comparación de Modelos')\n",
                "plt.show()\n",
                "\n",
                "# ROC Curves\n",
                "plt.figure(figsize=(10, 8))\n",
                "for name, res in results.items():\n",
                "    if res['y_prob'] is not None:\n",
                "        fpr, tpr, _ = roc_curve(y_test, res['y_prob'])\n",
                "        roc_auc = auc(fpr, tpr)\n",
                "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
                "\n",
                "plt.plot([0, 1], [0, 1], 'k--')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curves')\n",
                "plt.legend(loc='lower right')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Matrices de Confusión\n",
                "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, (name, res) in enumerate(results.items()):\n",
                "    if i >= len(axes): break\n",
                "    cm = confusion_matrix(y_test, res['y_pred'])\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
                "    axes[i].set_title(f'Confusion Matrix: {name}')\n",
                "    axes[i].set_xlabel('Predicted')\n",
                "    axes[i].set_ylabel('True')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}